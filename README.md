# ğŸŒ¸ **Data Preparation â€” MLOps Iris Classifier**

This branch builds upon the **initial setup** by introducing the **`data_processing.py`** module inside `src/`.
It marks the **first executable workflow stage** of the **MLOps Iris Classifier** pipeline â€” responsible for loading the Iris dataset, handling outliers, splitting into train/test, and persisting processed artefacts.

## ğŸ§© **Overview**

The `DataProcessing` class implements a **reproducible preprocessing pipeline** with integrated logging and exception handling.
It produces clean, split datasets ready for model training.

### ğŸ” Core Responsibilities

| Stage | Operation           | Description                                                                  |
| ----: | ------------------- | ---------------------------------------------------------------------------- |
|   1ï¸âƒ£ | **Load Data**       | Reads input CSV from `artifacts/raw/data.csv`.                               |
|   2ï¸âƒ£ | **Handle Outliers** | Applies the 1.5 Ã— IQR rule to `SepalWidthCm`; replaces outliers with median. |
|   3ï¸âƒ£ | **Split Data**      | Creates 80/20 train/test splits for features and target (`Species`).         |
|   4ï¸âƒ£ | **Save Artefacts**  | Writes `X_train.pkl`, `X_test.pkl`, `y_train.pkl`, `y_test.pkl` to disk.     |

## ğŸ—‚ï¸ **Updated Project Structure**

```text
mlops_iris_classifier/
â”œâ”€â”€ .venv/                          # ğŸ§© Local virtual environment (created by uv)
â”œâ”€â”€ artifacts/
â”‚   â”œâ”€â”€ raw/
â”‚   â”‚   â””â”€â”€ data.csv                # ğŸŒ¸ Input Iris dataset
â”‚   â””â”€â”€ processed/                  # ğŸ’¾ Output directory for processed data
â”‚       â”œâ”€â”€ X_train.pkl
â”‚       â”œâ”€â”€ X_test.pkl
â”‚       â”œâ”€â”€ y_train.pkl
â”‚       â””â”€â”€ y_test.pkl
â”œâ”€â”€ mlops_iris_classifier.egg-info/ # ğŸ“¦ Package metadata (auto-generated)
â”œâ”€â”€ pipeline/                       # âš™ï¸ Pipeline orchestration (future stage)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ custom_exception.py         # Unified and detailed exception handling
â”‚   â”œâ”€â”€ logger.py                   # Centralised logging configuration
â”‚   â””â”€â”€ data_processing.py          # ğŸŒ¸ End-to-end Iris data preparation
â”œâ”€â”€ static/                         # ğŸŒ Visual assets (optional)
â”œâ”€â”€ templates/                      # ğŸ§© Placeholder for web/API templates
â”œâ”€â”€ .gitignore                      # ğŸš« Git ignore rules
â”œâ”€â”€ .python-version                 # ğŸ Python version pin
â”œâ”€â”€ pyproject.toml                  # âš™ï¸ Project metadata and uv configuration
â”œâ”€â”€ requirements.txt                # ğŸ“¦ Python dependencies
â”œâ”€â”€ setup.py                        # ğŸ”§ Editable install support
â””â”€â”€ uv.lock                         # ğŸ”’ Locked dependency versions
```

## âš™ï¸ **How to Run the Data Processing Module**

After activating the virtual environment and ensuring your dataset is located at `artifacts/raw/data.csv`, run:

```bash
python src/data_processing.py
```

### âœ… **Expected Successful Output**

```console
2025-11-06 23:10:06,743 - INFO - Data read successfully. Shape: (150, 6)
2025-11-06 23:10:06,744 - INFO - Starting outlier handling for column: SepalWidthCm
2025-11-06 23:10:06,747 - INFO - Outliers handled successfully for column: SepalWidthCm
2025-11-06 23:10:06,749 - INFO - Data split successfully into train/test.
2025-11-06 23:10:06,755 - INFO - Processed files saved successfully for data-processing step.
```

This confirms that:

* The data was read successfully.
* Outliers were detected and replaced using the IQR rule.
* Train/test splits were created and saved under `artifacts/processed/`.

## ğŸ§  **Implementation Highlights**

* **Integrated Logging** via `src/logger.py`
  Every major step produces timestamped logs for full traceability.

* **Unified Exception Handling** via `src/custom_exception.py`
  Failures are raised with consistent, contextual messages for quicker debugging.

* **Minimal, Modular Design**
  The `DataProcessing` class is importable and ready for integration with future stages (training, evaluation, pipelines).

## ğŸ§© **Integration Guidelines**

| File                      | Purpose                                                   |
| ------------------------- | --------------------------------------------------------- |
| `src/data_processing.py`  | Executes the Iris preprocessing workflow end-to-end.      |
| `src/custom_exception.py` | Provides consistent, traceable error reporting.           |
| `src/logger.py`           | Ensures structured, timestamped logs for reproducibility. |

âœ… **In summary:**
This branch converts the repository from a static scaffold into a **functional preprocessing stage** for the Iris classifier â€” with reproducible outputs, consistent logging, and robust error handling.
Run it with `python src/data_processing.py` to generate training-ready artefacts for the next steps in the MLOps pipeline.